{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ab0d1d1-0220-4785-8a26-659f88340386",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler  # to standardize the features\n",
    "from sklearn.decomposition import PCA  # to apply PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1e33106-e0b9-41d1-8196-cfb528f582de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OS-agnostic file path\n",
    "working_dir = Path.cwd()\n",
    "data_location = Path(working_dir, 'data', 'input')\n",
    "app = pd.read_csv(data_location / 'application_train.csv')\n",
    "bureau = pd.read_csv(data_location / 'bureau.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5fe00c84-024a-4d3d-bbeb-a27769c43570",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = app.merge(bureau, how='left', on='SK_ID_CURR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aad2fe5-08f2-4d24-a3f0-144dd29d7b81",
   "metadata": {},
   "source": [
    "# First, we will handle the missing features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f21239dd-779a-4da1-8dd9-a3d95421c70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I try to separate numeric and categorical data. However, in the numeric data set there still are \n",
    "# categorical features, like CNT_CHILDREN. Those will have to be tackled differently.\n",
    "numerical_data = dataset.select_dtypes(include=[np.number])\n",
    "categorial_data = dataset.select_dtypes(exclude=[np.number])\n",
    "numerical_data = numerical_data.drop(columns=['SK_ID_CURR', 'TARGET'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "011385a8-22f5-4766-96a0-3087e49a5117",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in numerical_data.columns:\n",
    "    categorial = []\n",
    "    if len(pd.unique(numerical_data[feature])) > 20: # after 20 unique values, we can safely assume it is not categorial\n",
    "        median = numerical_data[feature].median()\n",
    "        numeric_data = numerical_data.fillna({feature: median}, inplace=True)\n",
    "    else:\n",
    "        categorial.append(feature)\n",
    "\n",
    "# Here we one-hot-encode the numeric categorical data\n",
    "numeric_data_encoded = pd.get_dummies(data=numerical_data, columns=categorial, drop_first=True)\n",
    "numeric_data_encoded.insert(0, 'SK_ID_CURR', dataset['SK_ID_CURR'])\n",
    "\n",
    "# Now we can finally separate numerical and categorial features\n",
    "numerical_data = numerical_data.drop(columns=categorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3fc88fa3-8024-4b87-bdef-654f0fee5271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we scale the numeric data using StandardScaler\n",
    "ss = StandardScaler()\n",
    "scaled_numeric_data = pd.DataFrame(ss.fit_transform(numerical_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a3652f18-e999-4b73-844d-4d0e5f35632e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here we one-hot-encode the categorical data\n",
    "\n",
    "categorial_data_encoded = pd.get_dummies(data=categorial_data, drop_first=True)\n",
    "categorial_data_encoded.insert(0, 'SK_ID_CURR', dataset['SK_ID_CURR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4aa794-1337-4def-9227-ea553ee1d2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell outputs the correlation matrix to an html file.\n",
    "\n",
    "# corr = numeric_data.corr()\n",
    "# with open('corrmatrix.html', 'a') as f:\n",
    "#     f.write(corr.style.background_gradient(cmap='coolwarm').format(precision=2).to_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "92e81df3-c467-4e90-b2d0-afa59d83176f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will reduce the dimensionality of the datasets using PCA\n",
    "\n",
    "pca = PCA(n_components=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7dc6eeb7-a951-4669-b20f-07922907d205",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_data = pca.fit_transform(categorial_data_encoded)\n",
    "cat_data = pd.DataFrame(cat_data)\n",
    "cat_data.insert(0, 'SK_ID_CURR', dataset['SK_ID_CURR'])\n",
    "cat_data.insert(1, 'TARGET', dataset['TARGET'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f45aaf93-8892-481e-8870-f882c68ed50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_numeric_data = scaled_numeric_data.dropna()\n",
    "num_data = pca.fit_transform(scaled_numeric_data)\n",
    "num_data = pd.DataFrame(num_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "000d7834-db3c-4f19-a042-aaac1f9e8f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_data_encoded = numeric_data_encoded.dropna()\n",
    "num_cat_data = pca.fit_transform(numeric_data_encoded)\n",
    "num_cat_data = pd.DataFrame(num_cat_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8dd82977-9cc6-43a5-a562-752113a18efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ready_dataset = pd.concat([cat_data, num_cat_data, num_data], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "39ac0192-fb37-4e8c-88ce-f3ed16cf6b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ready_dataset = ready_dataset.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e66ee06b-6f24-4464-b3fe-718da532e9c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_4196\\2131147869.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      4\u001b[39m X_train = ready_dataset.drop(columns=[\u001b[33m'SK_ID_CURR'\u001b[39m, \u001b[33m'TARGET'\u001b[39m])\n\u001b[32m      5\u001b[39m y_train = ready_dataset[\u001b[33m'TARGET'\u001b[39m]\n\u001b[32m      6\u001b[39m \n\u001b[32m      7\u001b[39m model = XGBClassifier(use_label_encoder=\u001b[38;5;28;01mFalse\u001b[39;00m, eval_metric=\u001b[33m'logloss'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m model.fit(X_train, y_train)\n",
      "\u001b[32m~\\Documents\\jupyter_project\\.venv\\Lib\\site-packages\\xgboost\\core.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    725\u001b[39m                     \u001b[38;5;28;01mraise\u001b[39;00m TypeError(msg)\n\u001b[32m    726\u001b[39m                 warnings.warn(msg, FutureWarning)\n\u001b[32m    727\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;28;01min\u001b[39;00m zip(sig.parameters, args):\n\u001b[32m    728\u001b[39m                 kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m func(**kwargs)\n",
      "\u001b[32m~\\Documents\\jupyter_project\\.venv\\Lib\\site-packages\\xgboost\\sklearn.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[39m\n\u001b[32m   1659\u001b[39m \n\u001b[32m   1660\u001b[39m             model, metric, params, feature_weights = self._configure_fit(\n\u001b[32m   1661\u001b[39m                 xgb_model, params, feature_weights\n\u001b[32m   1662\u001b[39m             )\n\u001b[32m-> \u001b[39m\u001b[32m1663\u001b[39m             train_dmatrix, evals = _wrap_evaluation_matrices(\n\u001b[32m   1664\u001b[39m                 missing=self.missing,\n\u001b[32m   1665\u001b[39m                 X=X,\n\u001b[32m   1666\u001b[39m                 y=y,\n",
      "\u001b[32m~\\Documents\\jupyter_project\\.venv\\Lib\\site-packages\\xgboost\\sklearn.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(missing, X, y, group, qid, sample_weight, base_margin, feature_weights, eval_set, sample_weight_eval_set, base_margin_eval_set, eval_group, eval_qid, create_dmatrix, enable_categorical, feature_types)\u001b[39m\n\u001b[32m    624\u001b[39m     feature_types: Optional[FeatureTypes],\n\u001b[32m    625\u001b[39m ) -> Tuple[Any, List[Tuple[Any, str]]]:\n\u001b[32m    626\u001b[39m     \"\"\"Convert array_like evaluation matrices into DMatrix.  Perform validation on the\n\u001b[32m    627\u001b[39m     way.\"\"\"\n\u001b[32m--> \u001b[39m\u001b[32m628\u001b[39m     train_dmatrix = create_dmatrix(\n\u001b[32m    629\u001b[39m         data=X,\n\u001b[32m    630\u001b[39m         label=y,\n\u001b[32m    631\u001b[39m         group=group,\n",
      "\u001b[32m~\\Documents\\jupyter_project\\.venv\\Lib\\site-packages\\xgboost\\sklearn.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, ref, **kwargs)\u001b[39m\n\u001b[32m   1136\u001b[39m             \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1137\u001b[39m                 return QuantileDMatrix(\n\u001b[32m   1138\u001b[39m                     **kwargs, ref=ref, nthread=self.n_jobs, max_bin=self.max_bin\n\u001b[32m   1139\u001b[39m                 )\n\u001b[32m-> \u001b[39m\u001b[32m1140\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m TypeError:  \u001b[38;5;66;03m# `QuantileDMatrix` supports lesser types than DMatrix\u001b[39;00m\n\u001b[32m   1141\u001b[39m                 \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   1142\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m DMatrix(**kwargs, nthread=self.n_jobs)\n",
      "\u001b[32m~\\Documents\\jupyter_project\\.venv\\Lib\\site-packages\\xgboost\\core.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    725\u001b[39m                     \u001b[38;5;28;01mraise\u001b[39;00m TypeError(msg)\n\u001b[32m    726\u001b[39m                 warnings.warn(msg, FutureWarning)\n\u001b[32m    727\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;28;01min\u001b[39;00m zip(sig.parameters, args):\n\u001b[32m    728\u001b[39m                 kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m func(**kwargs)\n",
      "\u001b[32m~\\Documents\\jupyter_project\\.venv\\Lib\\site-packages\\xgboost\\core.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, max_bin, ref, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical, max_quantile_batches, data_split_mode)\u001b[39m\n\u001b[32m   1610\u001b[39m                     \u001b[33m\"If data iterator is used as input, data like label should be \"\u001b[39m\n\u001b[32m   1611\u001b[39m                     \u001b[33m\"specified as batch argument.\"\u001b[39m\n\u001b[32m   1612\u001b[39m                 )\n\u001b[32m   1613\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m1614\u001b[39m         self._init(\n\u001b[32m   1615\u001b[39m             data,\n\u001b[32m   1616\u001b[39m             ref=ref,\n\u001b[32m   1617\u001b[39m             label=label,\n",
      "\u001b[32m~\\Documents\\jupyter_project\\.venv\\Lib\\site-packages\\xgboost\\core.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, data, ref, enable_categorical, max_quantile_blocks, **meta)\u001b[39m\n\u001b[32m   1674\u001b[39m             next_callback,\n\u001b[32m   1675\u001b[39m             config,\n\u001b[32m   1676\u001b[39m             ctypes.byref(handle),\n\u001b[32m   1677\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1678\u001b[39m         it.reraise()\n\u001b[32m   1679\u001b[39m         \u001b[38;5;66;03m# delay check_call to throw intermediate exception first\u001b[39;00m\n\u001b[32m   1680\u001b[39m         _check_call(ret)\n\u001b[32m   1681\u001b[39m         self.handle = handle\n",
      "\u001b[32m~\\Documents\\jupyter_project\\.venv\\Lib\\site-packages\\xgboost\\core.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    568\u001b[39m             \u001b[38;5;66;03m#  pylint 2.7.0 believes `self._exception` can be None even with `assert\u001b[39;00m\n\u001b[32m    569\u001b[39m             \u001b[38;5;66;03m#  isinstace`\u001b[39;00m\n\u001b[32m    570\u001b[39m             exc = self._exception\n\u001b[32m    571\u001b[39m             self._exception = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m572\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m exc  \u001b[38;5;66;03m# pylint: disable=raising-bad-type\u001b[39;00m\n",
      "\u001b[32m~\\Documents\\jupyter_project\\.venv\\Lib\\site-packages\\xgboost\\core.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, fn, dft_ret)\u001b[39m\n\u001b[32m    558\u001b[39m             tb = sys.exc_info()[\u001b[32m2\u001b[39m]\n\u001b[32m    559\u001b[39m             \u001b[38;5;66;03m# On dask, the worker is restarted and somehow the information is\u001b[39;00m\n\u001b[32m    560\u001b[39m             \u001b[38;5;66;03m# lost.\u001b[39;00m\n\u001b[32m    561\u001b[39m             self._exception = e.with_traceback(tb)\n\u001b[32m--> \u001b[39m\u001b[32m562\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m dft_ret\n",
      "\u001b[32m~\\Documents\\jupyter_project\\.venv\\Lib\\site-packages\\xgboost\\core.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m self._handle_exception(\u001b[38;5;28;01mlambda\u001b[39;00m: int(self.next(input_data)), \u001b[32m0\u001b[39m)\n",
      "\u001b[32m~\\Documents\\jupyter_project\\.venv\\Lib\\site-packages\\xgboost\\data.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, input_data)\u001b[39m\n\u001b[32m   1650\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m next(self, input_data: Callable) -> bool:\n\u001b[32m   1651\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m self.it == \u001b[32m1\u001b[39m:\n\u001b[32m   1652\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1653\u001b[39m         self.it += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1654\u001b[39m         input_data(**self.kwargs)\n\u001b[32m   1655\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[32m~\\Documents\\jupyter_project\\.venv\\Lib\\site-packages\\xgboost\\core.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    725\u001b[39m                     \u001b[38;5;28;01mraise\u001b[39;00m TypeError(msg)\n\u001b[32m    726\u001b[39m                 warnings.warn(msg, FutureWarning)\n\u001b[32m    727\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;28;01min\u001b[39;00m zip(sig.parameters, args):\n\u001b[32m    728\u001b[39m                 kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m func(**kwargs)\n",
      "\u001b[32m~\\Documents\\jupyter_project\\.venv\\Lib\\site-packages\\xgboost\\core.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(data, feature_names, feature_types, **kwargs)\u001b[39m\n\u001b[32m    616\u001b[39m                 \u001b[38;5;28;01mand\u001b[39;00m ref \u001b[38;5;28;01mis\u001b[39;00m self._data_ref\n\u001b[32m    617\u001b[39m             ):\n\u001b[32m    618\u001b[39m                 new, cat_codes, feature_names, feature_types = self._temporary_data\n\u001b[32m    619\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m                 new, cat_codes, feature_names, feature_types = _proxy_transform(\n\u001b[32m    621\u001b[39m                     data,\n\u001b[32m    622\u001b[39m                     feature_names,\n\u001b[32m    623\u001b[39m                     feature_types,\n",
      "\u001b[32m~\\Documents\\jupyter_project\\.venv\\Lib\\site-packages\\xgboost\\data.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(data, feature_names, feature_types, enable_categorical)\u001b[39m\n\u001b[32m   1703\u001b[39m             data, enable_categorical, feature_names, feature_types\n\u001b[32m   1704\u001b[39m         )\n\u001b[32m   1705\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m df_pa, \u001b[38;5;28;01mNone\u001b[39;00m, feature_names, feature_types\n\u001b[32m   1706\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_pandas_df(data):\n\u001b[32m-> \u001b[39m\u001b[32m1707\u001b[39m         df, feature_names, feature_types = _transform_pandas_df(\n\u001b[32m   1708\u001b[39m             data, enable_categorical, feature_names, feature_types\n\u001b[32m   1709\u001b[39m         )\n\u001b[32m   1710\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m df, \u001b[38;5;28;01mNone\u001b[39;00m, feature_names, feature_types\n",
      "\u001b[32m~\\Documents\\jupyter_project\\.venv\\Lib\\site-packages\\xgboost\\data.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(data, enable_categorical, feature_names, feature_types, meta)\u001b[39m\n\u001b[32m    640\u001b[39m     feature_names, feature_types = pandas_feature_info(\n\u001b[32m    641\u001b[39m         data, meta, feature_names, feature_types, enable_categorical\n\u001b[32m    642\u001b[39m     )\n\u001b[32m    643\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     arrays = pandas_transform_data(data)\n\u001b[32m    645\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m PandasTransformed(arrays), feature_names, feature_types\n",
      "\u001b[32m~\\Documents\\jupyter_project\\.venv\\Lib\\site-packages\\xgboost\\data.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    600\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m _is_np_array_like(arr):\n\u001b[32m    601\u001b[39m                 arr, _ = _ensure_np_dtype(arr, arr.dtype)\n\u001b[32m    602\u001b[39m             result.append(arr)\n\u001b[32m    603\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m             result.append(oth_type(data[col]))\n\u001b[32m    605\u001b[39m \n\u001b[32m    606\u001b[39m     \u001b[38;5;66;03m# FIXME(jiamingy): Investigate the possibility of using dataframe protocol or arrow\u001b[39;00m\n\u001b[32m    607\u001b[39m     \u001b[38;5;66;03m# IPC format for pandas so that we can apply the data transformation inside XGBoost\u001b[39;00m\n",
      "\u001b[32m~\\Documents\\jupyter_project\\.venv\\Lib\\site-packages\\xgboost\\data.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(ser)\u001b[39m\n\u001b[32m    566\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m oth_type(ser: PdSeries) -> np.ndarray:\n\u001b[32m    567\u001b[39m         \u001b[38;5;66;03m# The dtypes module is added in 1.25.\u001b[39;00m\n\u001b[32m    568\u001b[39m         npdtypes = np_dtypes and isinstance(\n\u001b[32m--> \u001b[39m\u001b[32m569\u001b[39m             ser.dtype,\n\u001b[32m    570\u001b[39m             (\n\u001b[32m    571\u001b[39m                 \u001b[38;5;66;03m# pylint: disable=no-member\u001b[39;00m\n\u001b[32m    572\u001b[39m                 np.dtypes.Float32DType,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "\u001b[32m~\\Documents\\jupyter_project\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   6314\u001b[39m             \u001b[38;5;28;01mand\u001b[39;00m name \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m self._accessors\n\u001b[32m   6315\u001b[39m             \u001b[38;5;28;01mand\u001b[39;00m self._info_axis._can_hold_identifiers_and_holds_name(name)\n\u001b[32m   6316\u001b[39m         ):\n\u001b[32m   6317\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self[name]\n\u001b[32m-> \u001b[39m\u001b[32m6318\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m object.__getattribute__(self, name)\n",
      "\u001b[31mAttributeError\u001b[39m: 'DataFrame' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "X_train = ready_dataset.drop(columns=['SK_ID_CURR', 'TARGET'])\n",
    "y_train = ready_dataset['TARGET']\n",
    "\n",
    "model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "model.fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
